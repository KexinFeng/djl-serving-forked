
processing spec_length = 10, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 3.51 reqs/sec, 
token_latency: 2.85 ms/token 
Peak memory usage (MiB): 70638.22021484375
Peak memory usage (including context) (MiB): 80513.2625
Avg accp length: 3.7input_size: 10
avg_time: 9.11,

processing spec_length = 9, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 3.69 reqs/sec, 
token_latency: 2.71 ms/token 
Peak memory usage (MiB): 70628.21953125
Peak memory usage (including context) (MiB): 80502.0625
Avg accp length: 3.5input_size: 10
avg_time: 8.66,

processing spec_length = 8, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 3.93 reqs/sec, 
token_latency: 2.54 ms/token 
Peak memory usage (MiB): 70618.2162109375
Peak memory usage (including context) (MiB): 80458.0625
Avg accp length: 3.4input_size: 10
avg_time: 8.13,

processing spec_length = 7, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 4.18 reqs/sec, 
token_latency: 2.39 ms/token 
Peak memory usage (MiB): 70607.559765625
Peak memory usage (including context) (MiB): 80452.0625
Avg accp length: 3.3input_size: 10
avg_time: 7.66,

processing spec_length = 6, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 4.38 reqs/sec, 
token_latency: 2.28 ms/token 
Peak memory usage (MiB): 70597.65048828124
Peak memory usage (including context) (MiB): 80438.0625
Avg accp length: 3.2input_size: 10
avg_time: 7.30,

processing spec_length = 5, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 4.72 reqs/sec, 
token_latency: 2.12 ms/token 
Peak memory usage (MiB): 70587.3259765625
Peak memory usage (including context) (MiB): 80434.0625
Avg accp length: 3.0input_size: 10
avg_time: 6.78,

processing spec_length = 4, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 4.99 reqs/sec, 
token_latency: 2 ms/token 
Peak memory usage (MiB): 70584.2642578125
Peak memory usage (including context) (MiB): 80414.0625
Avg accp length: 2.6input_size: 10
avg_time: 6.41,

processing spec_length = 3, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 4.9 reqs/sec, 
token_latency: 2.04 ms/token 
Peak memory usage (MiB): 70584.2642578125
Peak memory usage (including context) (MiB): 80412.0625
Avg accp length: 2.3input_size: 10
avg_time: 6.53,

processing spec_length = 2, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 4.3 reqs/sec, 
token_latency: 2.33 ms/token 
Peak memory usage (MiB): 70584.26328125
Peak memory usage (including context) (MiB): 80414.0625
Avg accp length: 1.7input_size: 10
avg_time: 7.44,

processing spec_length = 1, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 3.01 reqs/sec, 
token_latency: 3.33 ms/token 
Peak memory usage (MiB): 70584.26328125
Peak memory usage (including context) (MiB): 80410.0625
Avg accp length: 1.0input_size: 10
avg_time: 10.64,

processing spec_length = 0, draft_model = None .... 
tot_gen_tokens: 3200
seq_thru_put: 5.75 reqs/sec, 
token_latency: 1.74 ms/token 
Peak memory usage (MiB): 77135.0087890625
Peak memory usage (including context) (MiB): 80524.0625
Avg accp length: 1.0input_size: 10
avg_time: 5.56,
saved to /opt/mount_folder/djl-serving-forked/engines/python/setup/djl_python/tests/rolling_batch_test_scripts/data_v2_2/lmi_perform_concur_32+Llama-2-70B-Chat-fp16++TinyLlama-1.1B-Chat-v0.6.p
Time elapse: 671.5037896680005s
