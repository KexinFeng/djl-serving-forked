
processing spec_length = 10, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.14 reqs/sec, 
token_latency: 8.81 ms/token 
Peak memory usage (MiB): 70564.48173828125
Peak memory usage (including context) (MiB): 80380.0625
Avg accp length: 3.8input_size: 10
avg_time: 7.05,

processing spec_length = 9, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.19 reqs/sec, 
token_latency: 8.39 ms/token 
Peak memory usage (MiB): 70561.31962890625
Peak memory usage (including context) (MiB): 80381.6625
Avg accp length: 3.6input_size: 10
avg_time: 6.71,

processing spec_length = 8, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.12 reqs/sec, 
token_latency: 8.91 ms/token 
Peak memory usage (MiB): 70554.93779296875
Peak memory usage (including context) (MiB): 80382.0625
Avg accp length: 3.5input_size: 10
avg_time: 7.13,

processing spec_length = 7, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.3 reqs/sec, 
token_latency: 7.72 ms/token 
Peak memory usage (MiB): 70552.796875
Peak memory usage (including context) (MiB): 80382.0625
Avg accp length: 3.4input_size: 10
avg_time: 6.17,

processing spec_length = 6, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.34 reqs/sec, 
token_latency: 7.46 ms/token 
Peak memory usage (MiB): 70551.72626953125
Peak memory usage (including context) (MiB): 80382.0625
Avg accp length: 3.2input_size: 10
avg_time: 5.97,

processing spec_length = 5, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.43 reqs/sec, 
token_latency: 6.98 ms/token 
Peak memory usage (MiB): 70551.72705078125
Peak memory usage (including context) (MiB): 80380.0625
Avg accp length: 3.1input_size: 10
avg_time: 5.58,

processing spec_length = 4, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.46 reqs/sec, 
token_latency: 6.84 ms/token 
Peak memory usage (MiB): 70551.72578125
Peak memory usage (including context) (MiB): 80378.0625
Avg accp length: 2.7input_size: 10
avg_time: 5.47,

processing spec_length = 3, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.41 reqs/sec, 
token_latency: 7.09 ms/token 
Peak memory usage (MiB): 70551.72578125
Peak memory usage (including context) (MiB): 80376.0625
Avg accp length: 2.3input_size: 10
avg_time: 5.67,

processing spec_length = 2, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.06 reqs/sec, 
token_latency: 9.45 ms/token 
Peak memory usage (MiB): 70551.7248046875
Peak memory usage (including context) (MiB): 80378.0625
Avg accp length: 1.7input_size: 10
avg_time: 7.56,

processing spec_length = 1, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 0.717 reqs/sec, 
token_latency: 13.9 ms/token 
Peak memory usage (MiB): 70551.7248046875
Peak memory usage (including context) (MiB): 80376.0625
Avg accp length: 1.0input_size: 10
avg_time: 11.16,

processing spec_length = 0, draft_model = None .... 
tot_gen_tokens: 800
seq_thru_put: 1.36 reqs/sec, 
token_latency: 7.35 ms/token 
Peak memory usage (MiB): 77108.00927734375
Peak memory usage (including context) (MiB): 80520.0625
Avg accp length: 1.0input_size: 10
avg_time: 5.88,
saved to /opt/mount_folder/djl-serving-forked/engines/python/setup/djl_python/tests/rolling_batch_test_scripts/data_v2_2/lmi_perform_concur_8+Llama-2-70B-Chat-fp16++TinyLlama-1.1B-Chat-v0.6.p
Time elapse: 627.3772089959966s
