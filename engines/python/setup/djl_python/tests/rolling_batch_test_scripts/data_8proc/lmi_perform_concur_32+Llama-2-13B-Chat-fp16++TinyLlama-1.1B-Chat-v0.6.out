
processing spec_length = 10, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 5.2 reqs/sec, 
token_latency: 1.92 ms/token 
Peak memory usage (MiB): 60213.641276041664
Peak memory usage (including context) (MiB): 80764.3125
Avg accp length: 4.2input_size: 10
avg_time: 6.15,

processing spec_length = 9, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 5.4 reqs/sec, 
token_latency: 1.85 ms/token 
Peak memory usage (MiB): 60203.593424479164
Peak memory usage (including context) (MiB): 80754.3125
Avg accp length: 4.1input_size: 10
avg_time: 5.93,

processing spec_length = 8, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.03 reqs/sec, 
token_latency: 1.66 ms/token 
Peak memory usage (MiB): 60193.091471354164
Peak memory usage (including context) (MiB): 80710.3125
Avg accp length: 3.9input_size: 10
avg_time: 5.31,

processing spec_length = 7, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.49 reqs/sec, 
token_latency: 1.54 ms/token 
Peak memory usage (MiB): 60182.981608072914
Peak memory usage (including context) (MiB): 80704.3125
Avg accp length: 3.7input_size: 10
avg_time: 4.93,

processing spec_length = 6, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.44 reqs/sec, 
token_latency: 1.55 ms/token 
Peak memory usage (MiB): 60173.026529947914
Peak memory usage (including context) (MiB): 80700.3125
Avg accp length: 3.4input_size: 10
avg_time: 4.97,

processing spec_length = 5, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.72 reqs/sec, 
token_latency: 1.49 ms/token 
Peak memory usage (MiB): 60163.123697916664
Peak memory usage (including context) (MiB): 80698.3125
Avg accp length: 3.2input_size: 10
avg_time: 4.76,

processing spec_length = 4, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.6 reqs/sec, 
token_latency: 1.52 ms/token 
Peak memory usage (MiB): 60159.577799479164
Peak memory usage (including context) (MiB): 80682.3125
Avg accp length: 2.8input_size: 10
avg_time: 4.85,

processing spec_length = 3, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.54 reqs/sec, 
token_latency: 1.53 ms/token 
Peak memory usage (MiB): 60159.5771484375
Peak memory usage (including context) (MiB): 80662.3125
Avg accp length: 2.4input_size: 10
avg_time: 4.89,

processing spec_length = 2, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.46 reqs/sec, 
token_latency: 1.55 ms/token 
Peak memory usage (MiB): 60159.576171875
Peak memory usage (including context) (MiB): 80660.3125
Avg accp length: 1.8input_size: 10
avg_time: 4.95,

processing spec_length = 1, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 4.28 reqs/sec, 
token_latency: 2.34 ms/token 
Peak memory usage (MiB): 60159.576171875
Peak memory usage (including context) (MiB): 80658.3125
Avg accp length: 1.0input_size: 10
avg_time: 7.47,

processing spec_length = 0, draft_model = None .... 
tot_gen_tokens: 3200
seq_thru_put: 8.35 reqs/sec, 
token_latency: 1.2 ms/token 
Peak memory usage (MiB): 75469.55859375
Peak memory usage (including context) (MiB): 80428.3125
Avg accp length: 1.0input_size: 10
avg_time: 3.83,
saved to /opt/mount_folder/djl-serving-forked/engines/python/setup/djl_python/tests/rolling_batch_test_scripts/data/lmi_perform_concur_32+Llama-2-13B-Chat-fp16++TinyLlama-1.1B-Chat-v0.6.p
Time elapse: 261.4750616370002s
