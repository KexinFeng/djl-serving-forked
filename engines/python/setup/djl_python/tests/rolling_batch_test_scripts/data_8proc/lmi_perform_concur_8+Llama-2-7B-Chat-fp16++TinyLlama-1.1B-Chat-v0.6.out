
processing spec_length = 10, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.24 reqs/sec, 
token_latency: 8.06 ms/token 
Peak memory usage (MiB): 60337.8291015625
Peak memory usage (including context) (MiB): 80618.3125
Avg accp length: 3.8input_size: 10
avg_time: 6.45,

processing spec_length = 9, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.32 reqs/sec, 
token_latency: 7.55 ms/token 
Peak memory usage (MiB): 60335.387369791664
Peak memory usage (including context) (MiB): 80618.3125
Avg accp length: 3.6input_size: 10
avg_time: 6.04,

processing spec_length = 8, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.41 reqs/sec, 
token_latency: 7.12 ms/token 
Peak memory usage (MiB): 60333.413736979164
Peak memory usage (including context) (MiB): 80598.3125
Avg accp length: 3.6input_size: 10
avg_time: 5.69,

processing spec_length = 7, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.53 reqs/sec, 
token_latency: 6.52 ms/token 
Peak memory usage (MiB): 60333.41259765625
Peak memory usage (including context) (MiB): 80596.3125
Avg accp length: 3.4input_size: 10
avg_time: 5.22,

processing spec_length = 6, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.64 reqs/sec, 
token_latency: 6.09 ms/token 
Peak memory usage (MiB): 60328.06201171875
Peak memory usage (including context) (MiB): 80598.3125
Avg accp length: 3.3input_size: 10
avg_time: 4.87,

processing spec_length = 5, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.77 reqs/sec, 
token_latency: 5.65 ms/token 
Peak memory usage (MiB): 60325.6240234375
Peak memory usage (including context) (MiB): 80596.3125
Avg accp length: 3.1input_size: 10
avg_time: 4.52,

processing spec_length = 4, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.95 reqs/sec, 
token_latency: 5.13 ms/token 
Peak memory usage (MiB): 60325.24951171875
Peak memory usage (including context) (MiB): 80596.3125
Avg accp length: 2.8input_size: 10
avg_time: 4.11,

processing spec_length = 3, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 2.3 reqs/sec, 
token_latency: 4.35 ms/token 
Peak memory usage (MiB): 60325.24951171875
Peak memory usage (including context) (MiB): 80596.3125
Avg accp length: 2.3input_size: 10
avg_time: 3.48,

processing spec_length = 2, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 2.25 reqs/sec, 
token_latency: 4.45 ms/token 
Peak memory usage (MiB): 60325.24853515625
Peak memory usage (including context) (MiB): 80598.3125
Avg accp length: 1.8input_size: 10
avg_time: 3.56,

processing spec_length = 1, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 800
seq_thru_put: 1.67 reqs/sec, 
token_latency: 5.99 ms/token 
Peak memory usage (MiB): 60325.24853515625
Peak memory usage (including context) (MiB): 80596.3125
Avg accp length: 1.0input_size: 10
avg_time: 4.79,

processing spec_length = 0, draft_model = None .... 
tot_gen_tokens: 800
seq_thru_put: 3.56 reqs/sec, 
token_latency: 2.81 ms/token 
Peak memory usage (MiB): 75622.08349609375
Peak memory usage (including context) (MiB): 80486.3125
Avg accp length: 1.0input_size: 10
avg_time: 2.25,
saved to /opt/mount_folder/djl-serving-forked/engines/python/setup/djl_python/tests/rolling_batch_test_scripts/data/lmi_perform_concur_8+Llama-2-7B-Chat-fp16++TinyLlama-1.1B-Chat-v0.6.p
Time elapse: 224.51515506400028s
