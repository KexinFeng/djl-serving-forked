
processing spec_length = 5, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 7.15 reqs/sec, 
token_latency: 1.4 ms/token 
Peak memory usage (MiB): 60163.123697916664
Peak memory usage (including context) (MiB): 80698.3125
Avg accp length: 3.225324027916251input_size: 5
avg_time: 4.4729783923330615,

processing spec_length = 4, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.51 reqs/sec, 
token_latency: 1.54 ms/token 
Peak memory usage (MiB): 60159.577799479164
Peak memory usage (including context) (MiB): 80682.3125
Avg accp length: 2.8263525305410124input_size: 5
avg_time: 4.914462989333515,

processing spec_length = 3, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 6.52 reqs/sec, 
token_latency: 1.53 ms/token 
Peak memory usage (MiB): 60159.5771484375
Peak memory usage (including context) (MiB): 80662.3125
Avg accp length: 2.3686015184913054input_size: 5
avg_time: 4.911422257666648,

processing spec_length = 2, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 5.85 reqs/sec, 
token_latency: 1.71 ms/token 
Peak memory usage (MiB): 60159.576171875
Peak memory usage (including context) (MiB): 80660.3125
Avg accp length: 1.7790869792032116input_size: 5
avg_time: 5.471451844666262,

processing spec_length = 1, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 3200
seq_thru_put: 4.78 reqs/sec, 
token_latency: 2.09 ms/token 
Peak memory usage (MiB): 60159.576171875
Peak memory usage (including context) (MiB): 80658.3125
Avg accp length: 1.0input_size: 5
avg_time: 6.694162502000229,

processing spec_length = 0, draft_model = None .... 
tot_gen_tokens: 3200
seq_thru_put: 8.36 reqs/sec, 
token_latency: 1.2 ms/token 
Peak memory usage (MiB): 75469.55859375
Peak memory usage (including context) (MiB): 80428.3125
Avg accp length: 1.0input_size: 5
avg_time: 3.826154128666834,
saved to /opt/mount_folder/djl-serving-forked/engines/python/setup/djl_python/tests/rolling_batch_test_scripts/data/lmi_perform_concur_32+Llama-2-13B-Chat-fp16++TinyLlama-1.1B-Chat-v0.6.p
Time elapse: 135.20786110000063s
