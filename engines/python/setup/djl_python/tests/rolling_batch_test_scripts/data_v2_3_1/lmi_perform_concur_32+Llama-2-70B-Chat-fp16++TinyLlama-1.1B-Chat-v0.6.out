
processing spec_length = 7, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 8192
seq_thru_put: 0.593 reqs/sec, 
token_latency: 6.59 ms/token 
Peak memory usage (MiB): 21605.830403645832
Peak memory usage (including context) (MiB): 22448.0
Avg accp length: 3.7 
input_size: 7
avg_time: 53.99,

processing spec_length = 6, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 8192
seq_thru_put: 0.623 reqs/sec, 
token_latency: 6.27 ms/token 
Peak memory usage (MiB): 21605.141927083332
Peak memory usage (including context) (MiB): 22452.0
Avg accp length: 3.4 
input_size: 7
avg_time: 51.40,

processing spec_length = 5, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 8192
seq_thru_put: 0.663 reqs/sec, 
token_latency: 5.89 ms/token 
Peak memory usage (MiB): 21605.141927083332
Peak memory usage (including context) (MiB): 22452.0
Avg accp length: 3.2 
input_size: 7
avg_time: 48.24,

processing spec_length = 4, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 8192
seq_thru_put: 0.685 reqs/sec, 
token_latency: 5.7 ms/token 
Peak memory usage (MiB): 21605.140950520832
Peak memory usage (including context) (MiB): 22452.0
Avg accp length: 2.8 
input_size: 7
avg_time: 46.71,

processing spec_length = 3, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 8192
seq_thru_put: 0.733 reqs/sec, 
token_latency: 5.33 ms/token 
Peak memory usage (MiB): 21605.140462239582
Peak memory usage (including context) (MiB): 22452.0
Avg accp length: 2.4 
input_size: 7
avg_time: 43.67,

processing spec_length = 2, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 8192
seq_thru_put: 0.732 reqs/sec, 
token_latency: 5.34 ms/token 
Peak memory usage (MiB): 21605.139485677082
Peak memory usage (including context) (MiB): 22450.0
Avg accp length: 1.8 
input_size: 7
avg_time: 43.73,

processing spec_length = 1, draft_model = TinyLlama/TinyLlama-1.1B-Chat-v0.6 .... 
tot_gen_tokens: 8192
seq_thru_put: 0.582 reqs/sec, 
token_latency: 6.71 ms/token 
Peak memory usage (MiB): 21605.139485677082
Peak memory usage (including context) (MiB): 22448.0
Avg accp length: 1.0 
input_size: 7
avg_time: 55.00,

processing spec_length = 0, draft_model = None .... 
tot_gen_tokens: 8192
seq_thru_put: 0.801 reqs/sec, 
token_latency: 4.87 ms/token 
Peak memory usage (MiB): 21496.80419921875
Peak memory usage (including context) (MiB): 22338.0
Avg accp length: 1.0 
input_size: 7
avg_time: 39.93,
saved to /opt/mount_folder/djl-serving/engines/python/setup/djl_python/tests/rolling_batch_test_scripts/data_v2_3_1/lmi_perform_concur_32+Llama-2-70B-Chat-fp16++TinyLlama-1.1B-Chat-v0.6.p
Time elapse: 1317.351287903999s
